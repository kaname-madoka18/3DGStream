{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:58:46.724581700Z",
     "start_time": "2024-10-22T09:58:45.793302900Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_directory = '..'\n",
    "sys.path.append(os.path.abspath(project_directory))\n",
    "\n",
    "# import tinycudann as tcnn\n",
    "from ntc_models.encoding import NetworkWithInputEncoding\n",
    "import commentjson as ctjs\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from plyfile import PlyData, PlyElement\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:58:46.728092700Z",
     "start_time": "2024-10-22T09:58:46.721693200Z"
    }
   },
   "outputs": [],
   "source": [
    "postfixs=['BiRF']\n",
    "ntc_conf_paths=['../configs/cache/cache_'+postfix+'.json' for postfix in postfixs]\n",
    "pcd_path='../test/cook_spinach/init/point_cloud/iteration_15000/point_cloud.ply'\n",
    "save_paths=['../ntc/cook_spinach_ntc_params_'+postfix+'.pth' for postfix in postfixs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:58:47.588279400Z",
     "start_time": "2024-10-22T09:58:46.728092700Z"
    }
   },
   "outputs": [],
   "source": [
    "class BasicPointCloud(NamedTuple):\n",
    "    points : np.array\n",
    "    colors : np.array\n",
    "    normals : np.array\n",
    "\n",
    "def fetchXYZ(path):\n",
    "    plydata = PlyData.read(path)\n",
    "    xyz = np.stack((np.asarray(plydata.elements[0][\"x\"]),\n",
    "                    np.asarray(plydata.elements[0][\"y\"]),\n",
    "                    np.asarray(plydata.elements[0][\"z\"])),  axis=1)\n",
    "    return torch.tensor(xyz, dtype=torch.float, device=\"cuda\")\n",
    "\n",
    "def get_xyz_bound(xyz, percentile=80):\n",
    "    ## Hard-code the coordinate of the corners here!!\n",
    "    return torch.tensor([-20, -15,   5]).cuda(), torch.tensor([15, 10, 23]).cuda()\n",
    "\n",
    "def get_contracted_xyz(xyz):\n",
    "    xyz_bound_min, xyz_bound_max = get_xyz_bound(xyz, 80)\n",
    "    normalzied_xyz=(xyz-xyz_bound_min)/(xyz_bound_max-xyz_bound_min)\n",
    "    return normalzied_xyz\n",
    "\n",
    "@torch.compile\n",
    "def quaternion_multiply(a, b):\n",
    "    a_norm=nn.functional.normalize(a)\n",
    "    b_norm=nn.functional.normalize(b)\n",
    "    w1, x1, y1, z1 = a_norm[:, 0], a_norm[:, 1], a_norm[:, 2], a_norm[:, 3]\n",
    "    w2, x2, y2, z2 = b_norm[:, 0], b_norm[:, 1], b_norm[:, 2], b_norm[:, 3]\n",
    "\n",
    "    w = w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2\n",
    "    x = w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2\n",
    "    y = w1 * y2 + y1 * w2 + z1 * x2 - x1 * z2\n",
    "    z = w1 * z2 + z1 * w2 + x1 * y2 - y1 * x2\n",
    "\n",
    "    return torch.stack([w, x, y, z], dim=1)\n",
    "\n",
    "def quaternion_loss(q1, q2):\n",
    "    cos_theta = F.cosine_similarity(q1, q2, dim=1)\n",
    "    cos_theta = torch.clamp(cos_theta, -1+1e-7, 1-1e-7)\n",
    "    return 1-torch.pow(cos_theta, 2).mean()\n",
    "\n",
    "def l1loss(network_output, gt):\n",
    "    return torch.abs((network_output - gt)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:59:24.715762300Z",
     "start_time": "2024-10-22T09:58:47.591648300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7223, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.9998, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.9990, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.9984, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.9978, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.9970, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.9954, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.9922, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.9850, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.9660, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.8991, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.6122, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0425, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0201, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0100, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0047, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ntcs=[]\n",
    "for ntc_conf_path in ntc_conf_paths:    \n",
    "    # with open(ntc_conf_path) as ntc_conf_file:\n",
    "    #     ntc_conf = ctjs.load(ntc_conf_file)\n",
    "    # ntc=tcnn.NetworkWithInputEncoding(n_input_dims=3, n_output_dims=8, encoding_config=ntc_conf[\"encoding\"], network_config=ntc_conf[\"network\"]).to(torch.device(\"cuda\"))\n",
    "    ntc = NetworkWithInputEncoding(\n",
    "            n_input_dims=3,\n",
    "            n_output_dims=8,\n",
    "            network_config={\n",
    "                \"otype\": \"FullyFusedMLP\",\n",
    "                \"activation\": \"ReLU\",\n",
    "                \"output_activation\": \"None\",\n",
    "                \"n_neurons\": 128,\n",
    "                \"n_hidden_layers\": 1,\n",
    "            },\n",
    "            n_features_per_level=n_features_per_level,\n",
    "            # log2_hashmap_size=19,\n",
    "        )\n",
    "    ntc_optimizer = torch.optim.Adam(ntc.parameters(), lr=1e-4)\n",
    "    xyz=fetchXYZ(pcd_path)\n",
    "    normalzied_xyz=get_contracted_xyz(xyz)\n",
    "    mask = (normalzied_xyz >= 0) & (normalzied_xyz <= 1)\n",
    "    mask = mask.all(dim=1)\n",
    "    ntc_inputs=torch.cat([normalzied_xyz[mask]],dim=-1)\n",
    "    noisy_inputs = ntc_inputs + 0.01 * torch.rand_like(ntc_inputs)\n",
    "    d_xyz_gt=torch.tensor([0.,0.,0.]).cuda()\n",
    "    d_rot_gt=torch.tensor([1.,0.,0.,0.]).cuda()\n",
    "    dummy_gt=torch.tensor([1.]).cuda()\n",
    "    def cacheloss(resi):\n",
    "        masked_d_xyz=resi[:,:3]\n",
    "        masked_d_rot=resi[:,3:7]\n",
    "        masked_dummy=resi[:,7:8]\n",
    "        loss_xyz=l1loss(masked_d_xyz,d_xyz_gt)\n",
    "        loss_rot=quaternion_loss(masked_d_rot,d_rot_gt)\n",
    "        loss_dummy=l1loss(masked_dummy,dummy_gt)\n",
    "        loss=loss_xyz+loss_rot+loss_dummy\n",
    "        return loss\n",
    "    for iteration in range(0,3000):\n",
    "        ntc_inputs_w_noisy = torch.cat([noisy_inputs, ntc_inputs, torch.rand_like(ntc_inputs)],dim=0)  \n",
    "        ntc_output=ntc(ntc_inputs_w_noisy).to(torch.float64)\n",
    "        loss=cacheloss(ntc_output)\n",
    "        if iteration % 100 ==0:\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "        ntc_optimizer.step()\n",
    "        ntc_optimizer.zero_grad(set_to_none = True)\n",
    "    ntcs.append(ntc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T09:59:25.114267500Z",
     "start_time": "2024-10-22T09:59:24.717760500Z"
    }
   },
   "outputs": [],
   "source": [
    "from ntc import NeuralTransformationCache\n",
    "for idx, save_path in enumerate(save_paths):\n",
    "    ntc=NeuralTransformationCache(ntcs[idx],get_xyz_bound(xyz)[0],get_xyz_bound(xyz)[1])\n",
    "    torch.save(ntc.state_dict(),save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T09:59:25.169352100Z",
     "start_time": "2024-10-22T09:59:25.115269800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.0py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
